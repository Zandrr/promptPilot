<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ContextPilot</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    body {
      margin: 0;
      font-family: 'Inter', sans-serif;
      background-color: #0f0f0f;
      color: #f2f2f2;
      padding: 40px;
    }
    .container {
      max-width: 800px;
      margin: 0 auto;
    }
    h1 {
      font-size: 2.75rem;
      margin-bottom: 0.5rem;
    }
    h2 {
      font-size: 1.5rem;
      font-weight: 600;
      margin-top: 2rem;
      margin-bottom: 0.75rem;
    }
    p, ul {
      line-height: 1.6;
      margin-bottom: 1.25rem;
    }
    ul {
      margin-left: 1.25rem;
    }
    .cta {
      margin-top: 3rem;
    }
    .cta input {
      padding: 12px;
      border: none;
      border-radius: 6px;
      width: 250px;
      margin-right: 10px;
      font-size: 1rem;
    }
    .cta button {
      padding: 12px 20px;
      font-size: 1rem;
      border: none;
      border-radius: 6px;
      background-color: #32b67a;
      color: white;
      cursor: pointer;
    }
    .cta button:hover {
      background-color: #2ca86e;
    }
    .footer {
      margin-top: 5rem;
      font-size: 0.9rem;
      color: #777;
      text-align: center;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>ContextPilot</h1>
    <p><strong>Which context actually improves your LLM prompts?</strong></p>

    <h2>The Problem</h2>
    <p>LLM apps often rely on tons of stored data — user history, metadata, AI-generated summaries — but it’s unclear what’s actually helping the model respond better. Most teams over-inject context, waste tokens, and have no feedback loop.</p>

    <h2>The Opportunity</h2>
    <p>ContextPilot is a context optimization engine that shows you which data improves your prompt results — and which is just noise.</p>

    <h2>How It Works</h2>
    <ul>
      <li>Upload a JSON dataset or connect via API</li>
      <li>Define a prompt template and goal</li>
      <li>We auto-generate multiple context bundles using different strategies</li>
      <li>Each version is sent to your LLM provider (OpenAI, Claude, etc.)</li>
      <li>We log and compare output quality vs. token cost</li>
      <li>You see trade-offs and pick the best strategy to ship</li>
    </ul>

    <h2>Works alongside your existing stack</h2>
    <ul>
      <li><strong>Mem0 / Memory Tools:</strong> You already store user memories — we help decide which memories matter for this task.</li>
      <li><strong>PromptLayer / Prompt Management:</strong> You already manage prompts — we help optimize their inputs with real data.</li>
    </ul>

    <h2>For Teams That Care About:</h2>
    <ul>
      <li>Prompt cost vs. output quality</li>
      <li>Not overloading prompts with irrelevant context</li>
      <li>Faster iteration on what data drives better LLM responses</li>
    </ul>

    <div class="cta">
      <input type="email" placeholder="Your email" />
      <button>Join Early Access</button>
    </div>

    <div class="footer">
      Privacy-first. Model-agnostic. Built for high-context apps.
    </div>
  </div>
</body>
</html>