<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ContextPilot</title>
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <div class="container">
    <header class="hero">
      <h1>🧠 ContextPilot</h1>
      <p class="tagline">The prompt optimization platform that cuts costs and improves outputs for AI product teams.</p>
      <div class="cta">
        <input type="email" placeholder="Your email" />
        <button>Join Early Access</button>
      </div>
    </header>

    <section class="problems">
      <h2>❌ Context Should Be a Feature, Not a Hunch</h2>
      <div class="grid-3">
        <div class="card">
          <h3>🤯 Flying Blind: Most Context Goes Unused</h3>
          <p>You're injecting memory, metadata, summaries — but have no idea which elements actually improve outputs.</p>
        </div>
        <div class="card">
          <h3>💸 Token Waste: Ballooning LLM Costs</h3>
          <p>Tokens are cheaper, but costs add up fast when prompts are stuffed with "just in case" context.</p>
        </div>
        <div class="card">
          <h3>⚖️ Invisible Tradeoffs: Quality vs. Cost vs. Speed</h3>
          <p>No way to compare output quality, latency, and cost across strategies — until it hits production.</p>
        </div>
      </div>
    </section>

    <section class="solution-comparison">
      <h2>✅ From Guessing to Optimizing</h2>
      <div class="before-after">
        <div class="card">
          <h3>❌ Before ContextPilot</h3>
          <ul>
            <li>Hardcoded prompt logic with no way to test changes safely</li>
            <li>Manual copy/paste into playgrounds for tuning</li>
            <li>No clarity on token cost vs. output value</li>
            <li>Context added "just in case," not based on data</li>
            <li>Unknown impact on UX until users complain</li>
          </ul>
        </div>

        <div class="card highlight">
          <h3>✅ With ContextPilot</h3>
          <ul>
            <li>Modular prompt inputs treated like ML features</li>
            <li>Tests context variations automatically — real data, real outputs</li>
            <li>Side-by-side comparison of quality, latency, and cost</li>
            <li>Auto-identifies most effective context slice for each use case</li>
            <li>Deploys via SDK or config — no backend rewrites</li>
          </ul>
        </div>
      </div>
    </section>
    

    <section class="how-it-works">
      <h2>🚀 Deploy the Best Context with One Line of Code</h2>
      <p class="section-intro" style="text-align: center; margin-bottom: 30px; font-size: 1.2em;">
        <strong>ContextPilot runs the tests, ranks the results, and gives you clean, deployable output — no rewrites, no guesswork.</strong>
      </p>

      <div class="grid-3">
        <div class="card">
          <h3>🔗 1. Connect Your Context Sources</h3>
          <p>Upload your JSON or connect live sources. We’ll extract memory blocks, metadata, and past interactions for testing.</p>
        </div>

        <div class="card">
          <h3>🧠 2. Define Your Prompt</h3>
          <p>Use your real production prompt — just add variables like <code>{{user_query}}</code> and <code>{{context_block}}</code>. Nothing else changes.</p>
        </div>

        <div class="card">
          <h3>🔬 3. Auto-Generate Variants</h3>
          <p>ContextPilot builds multiple context strategies automatically and runs them through your selected model (OpenAI, Claude, etc.) with your real queries.</p>
        </div>

        <div class="card">
          <h3>📊 4. Compare and Choose</h3>
          <p>Each version is scored on output quality, latency, and cost. You get a side-by-side breakdown with a clear winner.</p>
        </div>

        <div class="card">
          <h3>🚀 5. Deploy via SDK or Export</h3>
          <p>Choose your path: dynamic fetching or static config.</p>

          <details class="code-toggle">
            <summary>Show example code</summary>
            <div class="code-block">
              <h4>▶️ Option A: Live Fetch with SDK</h4>
              <pre><code>
const context = await contextPilot.getContext({
  promptName: "support_reply",
  userId: "abc123"
});

const result = await llm.call({
  prompt: promptTemplate,
  context
});
              </code></pre>

              <h4>📦 Option B: Export Static Strategy</h4>
              <pre><code>
{
  "promptName": "support_reply",
  "contextBlocks": [
    "metadata.user_plan",
    "memory.last_3_messages",
    "summary.latest_issue"
  ]
}
              </code></pre>
            </div>
          </details>
        </div>
      </div>
    </section>

    <section class="use-cases">
      <h2>🎯 The Right Context for the Right Experience</h2>
      <div class="grid-3">
        <div class="card">
          <h3>🤖 AI Assistants</h3>
          <p>Decide what to include from memory, user metadata, and previous queries to improve usefulness without bloat, creating more responsive assistants.</p>
        </div>
        <div class="card">
          <h3>💬 Support Bots</h3>
          <p>Test whether injecting past tickets or user profile data actually leads to better answers — or just increases cost and response time.</p>
        </div>
        <div class="card">
          <h3>📚 Personalized Tools</h3>
          <p>Figure out which inputs (e.g. history, preferences, summaries) actually impact tone, clarity, and quality for truly personalized experiences.</p>
        </div>
      </div>
    </section>

    <section class="value-props">
      <h2>💡 What You Get with ContextPilot</h2>
      <div class="grid-4">
        <div class="card"><strong>💰 Lower Costs</strong><br />Trim context size drastically without losing output quality.</div>
        <div class="card"><strong>⚡ Faster Iteration</strong><br />Test multiple prompt variations using live data — no backend changes required.</div>
        <div class="card"><strong>🧠 Smarter Decisions</strong><br />Compare quality, latency, and token cost side-by-side — with real examples.</div>
        <div class="card"><strong>🤖 Better Product UX</strong><br />Build LLM features that feel intelligent and responsive, not bloated and slow.</div>
      </div>
    </section>

    <section class="security">
      <h2>🔒 Security and Privacy by Default</h2>
      <p>We don’t store your prompts, responses, or API keys beyond testing. Everything runs client-authenticated and encrypted end-to-end. You stay in full control of your data — always..</p>
    </section>

    <section class="compatibility">
      <h2>🔌 Plug Into Your Existing Stack</h2>
      <p>ContextPilot works out of the box with OpenAI, Anthropic, and Cohere. It also integrates cleanly with memory and orchestration tools like LangChain, LlamaIndex, and mem0. Use our REST API or export clean configs — no lock-in, no rewrites.</p>
    </section>

    <section class="team-note">
      <h2>🧪 Built by LLM Engineers — for LLM Teams</h2>
      <p>Created by an ex-Twitter engineer and Meta PM with extensive experience building AI products. We were tired of playground spaghetti and manual testing, so we built the prompt optimization tool we needed ourselves.</p>
    </section>

    <section class="cta">
      <h2>📬 Try It Yourself</h2>
      <p><strong>Wondering if your prompts are bloated, brittle, or just ineffective? Join our exclusive early access program today.</strong></p>
      <input type="email" placeholder="Your email" />
      <button>Join Early Access</button>
      <p class="small">Limited slots available. No credit card required.</p>
    </section>
  </div>
</body>
</html>
